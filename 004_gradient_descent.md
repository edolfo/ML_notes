# Formalism

A somewhat formal introduction can be found here:

http://www.deeplearningbook.org/

Take a look at part 1, chapter 4 (Numerical computation), section 4.3 on page 80.

# Informalism
[An excellent post for intuition](https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/)

[This post has some videos which illustrate](https://ikocabiyik.com/data-science/gradient-descent-with-momentum/):

* Why small learning rates can be expensive
* Why momentum can speed things up
* Why a [convex space](http://mathworld.wolfram.com/Convex.html) is so important for these optimisers.  When you find an optima in a convex space, you find a global optima!

# Further reading

http://neuralnetworksanddeeplearning.com/chap1.html#learning_with_gradient_descent
