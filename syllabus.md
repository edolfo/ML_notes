## What this is

A collection of notes going over:

* A top-down approach to statistical inference, wherein we first build intuition, then we go over mechanics.
* A place to help make the mathematics more approachable.  For you, not for me.  I know all the mathematics already.  All of it.
* Deep neural networks
* "Classical" machine learning (e.g. the ESL book)
* Some mathematical background required to understand whatever is covered
* How to actually implement these things in code!  This may include some of the dark art called "numerical computing".

In person this is:

* A place to ask any (relevant) question, no matter how elementary it may seem.  There are no stupid questions!
* A mechanism (e.g. forcing function) to encourage my own learning.  Oh, and yours as well...I guess.
* A mechanism (e.g. forcing function) to encourage my own DOING.  Oh, and yours too?  (For me, learning is typically
understanding the theory.  Your results may differ.)
* A process to learn new skills that are both relevant to my (your?) immediate future, and where the industry is going in the
next 5-10 years.

## What this is not

* A place to berate, castigate, look down upon, or otherwise pass telegraphed judgement on "stupid" questions.  There are no
stupid questions!
* A bottom-up approach to the broad field of machine learning.  You do not need to know measure theory, or topology, or
what an affine transformation is, or what the difference is between an isomorphism and a homomorphism, or, or, or, or...
* A place to learn how to program.  You should already know this.
* A place to learn top-tier American university entrance level mathematics (e.g. basic calculus).  You should already know this.

## Structure

In person, we will aim to meet twice a week.  One of the meetings will be for practice - typically this will mean coding. 
The other meeting will will be for theory, wherein we will go over a paper, or a blog post, or a mathematical concept.
You should show up prepared!  This means putting in some time to get the coding portions to work, or reading through the
paper.

The single most important thing for the success of this effort is your own individual effort, done consistently.  As a track
coach of mine was fond of saying, we do the ab workouts to get the best abs in the state!  Also, he said mediocre effort
yields mediocre results.  We don't want mediocrity - we want excellence.  Hold yourself to excellence!

Broadly, we will cover the following, in order:

1. Intuition around deep neural networks.  This means understanding how they work, what problem they solve, and what their
limitations are.
2. Understanding what a recurrent neural network with long/short term memory is.  This is a fairly standard architecture
that has yielded ridiculously good results across many applications.
3. Understanding the mathematics behind these networks
4. "Classical" machine learning, and how it ties into the neural networks that all the cool kids are using these days.

For practice, we will be following the following courses, in this order:

1. [Tensorflow in practice](https://www.coursera.org/specializations/tensorflow-in-practice)
2. [Deep learning specialisation](https://www.coursera.org/specializations/deep-learning)
